{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<br><h2>Team 2 | Unsupervised Analysis Project</h2>\n",
    "<h4>DAT-5303 | Machine Learning</h4>\n",
    "Michele Coletta, Tura Ventola Franch, Elnur Mammadov, Tumelo Seemule, Khushali Soni<br>\n",
    "Hult International Business School<br><br><br>\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Summary </h1>\n",
    "<br> The aim of this analysis is to analyze and segment app download users based on their purchase behavior, personality traits and technology usage. This analysis will further assist in creating and implementing promotional strategies that successfully reach the potential customer personas and has the potenital to increase usage and ultimately increasing revenue. </br>\n",
    "\n",
    "<br> The 5 different personas we identified are  Controllers, Tightwads, Techys, Wealthy Parents, and Self-concious.</br>\n",
    "\n",
    "<br>Amongst these top 5 which we identified, a few trends in behavior and app usage:\n",
    "1) Controllers and Tightwads had the most variance, which can be correlated with the fact that they contribute the most to the revenue of the mobile download industry. This can also be attributed to the fact that they spend so much time online and are so tech savy. They also are technologically advanced and always curious about the new and hot tech. You could probably find them sleeping outside the apple waiting for the new release.</br>\n",
    "<br>2) Investing in Digital Social Media Markteting would be wise as we identified that acorss all personas, on average social media platforms such as Facebook and Youtube are commonly used. Making them essential inorder to reach wider audiences and increase chances of more successful campaigns. However, the company should take into consideration that Instagram and Tiktok have become major avenues in advertizing and reaching millenials and Gen-Z in general.Despite the product being sold. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps Overview : \n",
    "    \n",
    "    1) Importing packages and Dataset\n",
    "    \n",
    "    2) Principal Component Analysis\n",
    "    \n",
    "    3) Interpreting Principal Components and Persona Development\n",
    "    \n",
    "    4) Exploring customers in the different personas\n",
    "    \n",
    "    5) Clustering\n",
    "    \n",
    "    6) Analysis with Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Importing necessary packages, loading data, and setting display options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#importing Packages\n",
    "import numpy as np # mathematical essentials\n",
    "import pandas as pd # data science essentials\n",
    "import matplotlib.pyplot as plt # fundamental data visualization\n",
    "import seaborn as sns # enhanced visualizations\n",
    "\n",
    "# packages for unsupervised learning\n",
    "from sklearn.preprocessing import StandardScaler # standard scaler\n",
    "from sklearn.decomposition import PCA # pca\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage # dendrograms\n",
    "from sklearn.cluster         import KMeans # k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "survey_data = pd.read_excel('./__datasets/Mobile_App_Survey_Data.xlsx')\n",
    "\n",
    "# setting print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>User-Defined Functions</strong><br>\n",
    "Loading the user-defined functions used throughout this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# scree_plot\n",
    "def scree_plot(pca_object, export = False):\n",
    "    \"\"\"\n",
    "    Visualizes a scree plot from a pca object.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    pca_object | A fitted pca object\n",
    "    export     | Set to True if you would like to save the scree plot to the\n",
    "               | current working directory (default: False)\n",
    "    \"\"\"\n",
    "    # building a scree plot\n",
    "\n",
    "    # setting plot size\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    features = range(pca_object.n_components_)\n",
    "\n",
    "\n",
    "    # developing a scree plot\n",
    "    plt.plot(features,\n",
    "             pca_object.explained_variance_ratio_,\n",
    "             linewidth = 2,\n",
    "             marker = 'o',\n",
    "             markersize = 10,\n",
    "             markeredgecolor = 'black',\n",
    "             markerfacecolor = 'grey')\n",
    "\n",
    "\n",
    "    # setting more plot options\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('PCA feature')\n",
    "    plt.ylabel('Explained Variance')\n",
    "    plt.xticks(features)\n",
    "\n",
    "    if export == True:\n",
    "    \n",
    "        # exporting the plot\n",
    "        plt.savefig('./__analysis_images/top_customers_correlation_scree_plot.png')\n",
    "        \n",
    "    # displaying the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User defined function to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# unsupervised_scaler\n",
    "def unsupervised_scaler(df):\n",
    "    \"\"\"\n",
    "    Standardizes a dataset (mean = 0, variance = 1). Returns a new DataFrame.\n",
    "    Requires sklearn.preprocessing.StandardScaler()\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    df     | DataFrame to be used for scaling\n",
    "    \"\"\"\n",
    "\n",
    "    # INSTANTIATING a StandardScaler() object\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    # FITTING the scaler with the data\n",
    "    scaler.fit(df)\n",
    "\n",
    "\n",
    "    # TRANSFORMING our data after fit\n",
    "    x_scaled = scaler.transform(df)\n",
    "\n",
    "    \n",
    "    # converting scaled data into a DataFrame\n",
    "    new_df = pd.DataFrame(x_scaled)\n",
    "\n",
    "\n",
    "    # reattaching column names\n",
    "    new_df.columns = df.columns\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def unsupervised_scaler_row(df):\n",
    "    \"\"\"\n",
    "    Standardizes a dataset (mean = 0, variance = 1). Returns a new DataFrame.\n",
    "    Requires sklearn.preprocessing.StandardScaler()\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    df     | DataFrame to be used for scaling\n",
    "    \"\"\"\n",
    "\n",
    "    # INSTANTIATING a StandardScaler() object\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    # FITTING the scaler with the data\n",
    "    scaler.fit(df.transpose())\n",
    "\n",
    "\n",
    "    # TRANSFORMING our data after fit\n",
    "    x_scaled = scaler.transform(df.transpose())\n",
    "\n",
    "    \n",
    "    # converting scaled data into a DataFrame\n",
    "    new_df = pd.DataFrame(x_scaled)\n",
    "\n",
    "\n",
    "    # reattaching column names\n",
    "    #new_df.columns = df.columns\n",
    "    \n",
    "    return new_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# summary of decriptive statistics\n",
    "#survey_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# dropping question 2r10 because it says terminate in the questionary\n",
    "survey_data = survey_data.drop('q2r10', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic =['caseID','q1','q2r1','q2r2','q2r3','q2r4','q2r5','q2r6','q2r7','q2r8','q2r9','q4r1','q4r2','q4r3','q4r4','q4r5','q4r6','q4r7','q4r8','q4r9','q4r10','q4r11','q11','q12','q13r1','q13r2','q13r3','q13r4','q13r5','q13r6','q13r7','q13r8','q13r9','q13r10','q13r11','q13r12','q48','q49','q50r1','q50r2','q50r3','q50r4','q50r5','q54','q55','q56','q57']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# dropping demographic information\n",
    "psychometric = survey_data.drop(demographic, axis = 1)\n",
    "\n",
    "# transposing the dataset\n",
    "psychometric_rows = pd.DataFrame(np.transpose(psychometric))\n",
    "\n",
    "# applying the unsupervised_scaler function\n",
    "psychometric_scaled = unsupervised_scaler(df = psychometric_rows)\n",
    "\n",
    "# transposing to the original\n",
    "customers_scaled = pd.DataFrame(np.transpose(psychometric_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>We then decided to instantiate, fit and transform a PCA model with no limits of principal components in order to examine the data and determine how many components we would use in our analysis.</br>\n",
    "<br> The PCA is known as a dimension reduction method that is used to decrease large datasets, while maintaining all the features and information.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA object with no limit to principal components\n",
    "pca = PCA(n_components = None,\n",
    "            random_state = 219)\n",
    "\n",
    "# FITTING and TRANSFORMING the data\n",
    "customer_pca = pca.fit_transform(customers_scaled)\n",
    "\n",
    "# comparing dimensions of each DataFrame\n",
    "print(\"Original shape:\", customers_scaled.shape)\n",
    "print(\"PCA shape     :\", customer_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluating PCA Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to analyzing the factor loadings of each principal components: <br>1) We created the code below to check each component's explained variance ration. Keeping in mind that all explained variance rations should sum up to 1.0.<br>\n",
    "\n",
    "<br>2) Developed screeplot to examine the point in which there is a drop in explained variance. The elbow where the line becomes less steep. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = pca,\n",
    "          export = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# component number counter\n",
    "#component_number = 0\n",
    "#cumulative_variance = 0\n",
    "\n",
    "# looping over each principal component\n",
    "#for variance in pca.explained_variance_ratio_:\n",
    "#    component_number += 1\n",
    "#    cumulative_variance += variance\n",
    "    \n",
    "#    print(f\"PC {component_number}: {variance.round(3)}, CV: {cumulative_variance.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redoing pca with limited principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the PCA we initially analyzed the screeplot above and determined that we would utilize three components for each PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA object with X principal components\n",
    "pca = PCA(n_components = 5,\n",
    "            random_state = 219)\n",
    "\n",
    "# FITTING and TRANSFORMING the data\n",
    "customer_pca = pca.fit_transform(customers_scaled)\n",
    "\n",
    "# comparing dimensions of each DataFrame\n",
    "print(\"Original shape:\", customers_scaled.shape)\n",
    "print(\"PCA shape     :\", customer_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Principal Components and Persona Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code aims to interpret the meaning of each principal component by looking into features that are strongly correlated to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transposing pca components\n",
    "factor_loadings_df = pd.DataFrame(np.transpose(pca.components_.round(decimals = 2)))\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_df = factor_loadings_df.set_index(psychometric.columns)\n",
    "\n",
    "# checking the result\n",
    "print(factor_loadings_df)\n",
    "\n",
    "# saving to Excel\n",
    "factor_loadings_df.to_excel('customer_factor_loadings.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***a) Analyzing the PC factor loadings.***\n",
    "<br>Upon looking into the factors loadings we decided to develop the following persona names for each of the principal components.  We also renamed the teams columns accordingly.</br>\n",
    "\n",
    "<br> 0: Controllers\n",
    "- This group is categorized by their dependency and excessive amount technology usage, they are not very social people but they are always checking on their relatives and friends by using the internet and social media sites. We cannot tell them what to do, they are very self judging.</br>\n",
    "\n",
    "<br>1: Tightwads\n",
    "- They are not into shopping of any kind or spending money on unnecessary things. They also have very strong characters. Besides this, they like to come up with initiatives and are also not huge fans of the internet. They are usually very careful with how they are spending their money. </br>\n",
    " \n",
    "<br>2: Techys group\n",
    "- They love technology, always up to date with all tech reveals and gadgets. Music and TV shows are part of their lives and they are very into the internet. Looking at their personality, they don't want to be on the frontline in any situation. Furthermore, they are not active and they are opposed to new fashion trends or anything that makes them stand out. </br>\n",
    " \n",
    "<br>3: Wealthy Parents\n",
    "- They don't keep up with trends. Especially with technological advances and social media accounts. Moreover, they take the responsibility very seriously. This group is not techy but they will have the funds to buy your product. We have to come up with a special marketing approach for them. Think about their children, they have a lot of impact on their decisions.</br>\n",
    " \n",
    "<br>4: Self-conscious. \n",
    "- This group is mixed. They do believe in technology and are also social but they are very smart. They do care about what they spent their time on and care about their lifestyle. They will trust and follow trends and they don't mind spending the extra money to be up to date. We want to approach them in a way that our product will keep up with their personality.</br>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_df.columns = ['Controllers',\n",
    "                              'Tightwads',\n",
    "                              'Techys',\n",
    "                              'Wealthy Parents',\n",
    "                              'Self-conscious']\n",
    "\n",
    "# checking the result\n",
    "factor_loadings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming and Analyzing how much each customer fits into each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# analyzing factor strengths per customer\n",
    "factor_loadings = pca.transform(customers_scaled)\n",
    "\n",
    "# converting into a DataFrame \n",
    "customers_df = pd.DataFrame(factor_loadings)\n",
    "\n",
    "# renaming columns\n",
    "customers_df.columns = factor_loadings_df.columns\n",
    "\n",
    "# checking results\n",
    "customers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring customers in the different personas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next segment we explored the market potential for customers with a standard deviation of two and above across all the 5 personality types we had developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exploring customers in the Controllers persona\n",
    "# customers_df['Controllers'][customers_df['Controllers'] > 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# exploring customers in the Tightwads persona\n",
    "# customers_df['Tightwads'][customers_df['Tightwads'] > 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exploring customers in the Techys persona\n",
    "# customers_df['Techys'][customers_df['Techys'] > 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exploring customers in the Wealthy Parents persona\n",
    "# customers_df['Wealthy Parents'][customers_df['Wealthy Parents'] > 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exploring customers in the Self-conscious persona\n",
    "# customers_df['Self-conscious'][customers_df['Self-conscious'] > 2.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal with the following code is to get an idea as to how many clusters will be appropriate given our analysis of these tools, and then applying this number of clusters to a k-Means Clustering. Prior to conducting a KNN we had to scaler the data again as this method is bsd on distance and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***a) Creating a scaled version of the factor loadings dataset.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# applying the unsupervised_scaler function\n",
    "pca_scaled = unsupervised_scaler(df = customers_df)\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(np.var(customers_df), '\\n\\n')\n",
    "print(np.var(pca_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code aims to help us understand how many clusters to build using k-Means through a dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grouping data based on Ward distance\n",
    "# standard_mergings_ward = linkage(y = pca_scaled,\n",
    "#                                 method = 'ward',\n",
    "#                                 optimal_ordering = True)\n",
    "\n",
    "\n",
    "# setting plot size\n",
    "# fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# developing a dendrogram\n",
    "# dendrogram(Z = standard_mergings_ward,\n",
    "#           leaf_rotation = 90,\n",
    "#           leaf_font_size = 6)\n",
    "\n",
    "\n",
    "# rendering the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>We chose five clusters as they showed the most variance amongst the variables. Next, we named the cluster and started to develop back storied for ideal members of each group.</br>\n",
    "\n",
    "<br> The analysis of the endrogram revealed that we should focus on 5 main clusters</br>\n",
    "\n",
    "<br>***Cluster***</br>\n",
    "________________________________\n",
    "<br>0:                363</br>\n",
    "<br>1:                299</br>\n",
    "<br>2:                272</br>\n",
    "<br>3:                310</br>\n",
    "<br>4:                308</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a k-Means object with five clusters\n",
    "# customers_k_pca = KMeans(n_clusters   = 5,\n",
    "#                        random_state = 219)\n",
    "\n",
    "\n",
    "# fitting the object to the data\n",
    "# customers_k_pca.fit(pca_scaled)\n",
    "\n",
    "\n",
    "# converting the clusters to a DataFrame\n",
    "# customers_kmeans_pca = pd.DataFrame({'Cluster': customers_k_pca.labels_})\n",
    "\n",
    "\n",
    "# checking the results\n",
    "# print(customers_kmeans_pca.iloc[: , 0].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# storing cluster centers\n",
    "centroids_pca = customers_k_pca.cluster_centers_\n",
    "\n",
    "\n",
    "# converting cluster centers into a DataFrame\n",
    "centroids_pca_df = pd.DataFrame(centroids_pca)\n",
    "\n",
    "\n",
    "# renaming principal components\n",
    "centroids_pca_df.columns = ['Controllers',\n",
    "                              'Tightwads',\n",
    "                              'Techys',\n",
    "                              'Wealthy Parents',\n",
    "                              'Self-conscious']\n",
    "\n",
    "# checking results (clusters = rows, pc = columns)\n",
    "centroids_pca_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Demographic Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then decided to concatinate and call back all the columns, and add the new developed personas into one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# concatinating cluster memberships with principal components\n",
    "clst_pca_df = pd.concat([customers_kmeans_pca,\n",
    "                          customers_df],\n",
    "                          axis = 1)\n",
    "\n",
    "\n",
    "# concatenating demographic information with pca-clusters\n",
    "final_pca_cluster_df = pd.concat([survey_data.loc[ : , demographic],\n",
    "                                  clst_pca_df.round(decimals = 2)],\n",
    "                                  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "final_pca_cluster_df.columns = [\n",
    "'CaseID',\n",
    "'Age',\n",
    "'phone_iPhone',\n",
    "'phone_iPod',\n",
    "'phone_Android',\n",
    "'phone_BlackBerry',\n",
    "'phone_Nokia',\n",
    "'phone_Windows',\n",
    "'phone_HP',\n",
    "'phone_Tablet',\n",
    "'phone_Other',\n",
    "'app_Music',\n",
    "'app_tv_check',\n",
    "'app_Ent',\n",
    "'app_TV_show',\n",
    "'app_Gaming',\n",
    "'app_Social',\n",
    "'app_News',\n",
    "'app_Shop',\n",
    "'app_Spec',\n",
    "'app_Other',\n",
    "'app_None',\n",
    "'Num_apps',\n",
    "'Free_apps',\n",
    "'visit_fb ',\n",
    "'visit_twitter',\n",
    "'visit_myspace',\n",
    "'visit_pandora',\n",
    "'visit_vevo',\n",
    "'visit_youtube',\n",
    "'visit_AOL',\n",
    "'visit_lastfm',\n",
    "'visit_yahoo',\n",
    "'visit_IMBD',\n",
    "'visit_linkedin',\n",
    "'visit_netflix',\n",
    "'education',\n",
    "'marital_status',\n",
    "'no_child',\n",
    "'child_less6',\n",
    "'child6_12',\n",
    "'child13_17',\n",
    "'child_more18',\n",
    "'race',\n",
    "'hispanic',\n",
    "'salary',\n",
    "'sex', \n",
    "'Cluster',\n",
    "'Controllers',\n",
    "'Tightwads',\n",
    "'Techys',\n",
    "'Wealthy Parents',\n",
    "'Self-conscious']\n",
    "\n",
    "# checking the results\n",
    "final_pca_cluster_df.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code adds labels to categorical variables along as assigning the specific labels to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# renaming regions\n",
    "cluster = {0 : 'Cluster 1',\n",
    "           1 : 'Cluster 2',\n",
    "           2 : 'Cluster 3',\n",
    "           3 : 'Cluster 4',\n",
    "           4 : 'Cluster 5'}\n",
    "\n",
    "final_pca_cluster_df['Cluster'].replace(cluster, inplace = True)\n",
    "\n",
    "age = {     1: 'Under 18',\n",
    "            2: '18-24',\n",
    "            3: '25-29',\n",
    "            4: '30-34',\n",
    "            5: '35-39',\n",
    "            6: '40-44',\n",
    "            7: '45-49',\n",
    "            8: '50-54',\n",
    "            9: '55-59',\n",
    "            10:'60-64',\n",
    "            11:'65+' }\n",
    "\n",
    "final_pca_cluster_df['Age'].replace(age, inplace = True)\n",
    "\n",
    "num_apps = {1: '1-5',\n",
    "            2: '6-10',\n",
    "            3: '11-30',\n",
    "            4: '31+',\n",
    "            5: 'Unknown',\n",
    "            6: 'None'}\n",
    "\n",
    "final_pca_cluster_df['Num_apps'].replace(num_apps, inplace = True)\n",
    "\n",
    "free_apps = {  1: 'None',\n",
    "               2: '1%-25%',\n",
    "               3: '26%-50%',\n",
    "               4: '51%-75%',\n",
    "               5: '76%-99%',\n",
    "               6: 'All'}\n",
    "\n",
    "final_pca_cluster_df['Free_apps'].replace(free_apps, inplace = True)\n",
    "\n",
    "marrital_status = {  1: 'Married',\n",
    "                     2: 'Single',\n",
    "                     3: 'Single w Partner',\n",
    "                     4: 'Separated'}\n",
    "\n",
    "final_pca_cluster_df['marital_status'].replace(marrital_status, inplace = True)\n",
    "\n",
    "race = {            1: 'White',\n",
    "                    2: 'Black',\n",
    "                    3: 'Asian',\n",
    "                    4: 'NHOPI',\n",
    "                    5: 'AIAN',\n",
    "                    6: 'Other'}\n",
    "\n",
    "final_pca_cluster_df['race'].replace(race, inplace = True)\n",
    "\n",
    "salary = {1: 'Under $10K',\n",
    "          2: '$10k-$14,9K',\n",
    "          3: '$15k-$19,9K',\n",
    "          4: '$20k-$29,9K',\n",
    "          5: '$30k-$39,9K',\n",
    "          6: '$40k-$49,9K',\n",
    "          7: '$50k-$59,9K',\n",
    "          8: '$60k-$69,9K',\n",
    "          9: '$70k-$79,9K',\n",
    "          10: '$80k-$89,9K',\n",
    "          11: '$90k-$99,9K',\n",
    "          12: '$100k-$124,9K',\n",
    "          13: '$125k-$149,9K',\n",
    "          14: '$150k and over'\n",
    "                }\n",
    "\n",
    "final_pca_cluster_df['salary'].replace(salary, inplace = True)\n",
    "\n",
    "sex = {            1: 'Male',\n",
    "                   2: 'Female'}\n",
    "\n",
    "final_pca_cluster_df['sex'].replace(sex, inplace = True)\n",
    "\n",
    "hispanic = {            1: 'Yes',\n",
    "                        2: 'No'}\n",
    "\n",
    "final_pca_cluster_df['hispanic'].replace(hispanic, inplace = True)\n",
    "\n",
    "# adding a productivity step\n",
    "data_df = final_pca_cluster_df\n",
    "\n",
    "# checking results\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis with Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can analyze our results with demographics and other data. \n",
    "The code below displays some visuals with the relation of some of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Percentage of free apps\n",
    "########################\n",
    "\n",
    "lst = ['Controllers',\n",
    "       'Tightwads',\n",
    "       'Techys',\n",
    "       'Wealthy Parents',\n",
    "       'Self-conscious']\n",
    "\n",
    "for i in lst:\n",
    "    #plotting all the segments\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    sns.boxplot(x = 'Free_apps',\n",
    "                y = i,\n",
    "                hue = 'Cluster',\n",
    "                data = data_df,\n",
    "                palette = 'rocket',\n",
    "                order = ['None','1%-25%','26%-50%','51%-75%','76%-99%','All'])\n",
    "\n",
    "    # formatting and displaying the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Age\n",
    "########################\n",
    "\n",
    "lst = ['Controllers',\n",
    "       'Tightwads',\n",
    "       'Techys',\n",
    "       'Wealthy Parents',\n",
    "       'Self-conscious']\n",
    "\n",
    "for i in lst:\n",
    "    #plotting all the segments\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    sns.boxplot(x = 'Age',\n",
    "                y = i,\n",
    "                hue = 'Cluster',\n",
    "                data = data_df,\n",
    "                palette = 'rocket',\n",
    "                order = ['Under 18','18-24','25-29','30-34','35-39','40-44',\n",
    "                        '45-49','50-54','55-59','60-64','65+'])\n",
    "\n",
    "    # formatting and displaying the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Race\n",
    "########################\n",
    "\n",
    "lst = ['Controllers',\n",
    "       'Tightwads',\n",
    "       'Techys',\n",
    "       'Wealthy Parents',\n",
    "       'Self-conscious']\n",
    "\n",
    "for i in lst:\n",
    "    #plotting all the segments\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    sns.boxplot(x = 'race',\n",
    "                y = i,\n",
    "                hue = 'Cluster',\n",
    "                data = data_df,\n",
    "                palette = 'rocket')\n",
    "\n",
    "\n",
    "    # formatting and displaying the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Strategies for Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five groups identified are pretty different, and for any company to be able to market successfully to them, the marketing strategies implemented need to be specific:\n",
    "Controllers:\n",
    "\n",
    "- This is the group with the most variance, meaning that they can generate a tremendous amount of revenue for app companies. They are willing to pay for apps; however, recommend approaching them with a freemium subscription. This strategy would ensure that you build trust and give you constructive feedback and get to experience the apps. The assumption is that conversion rates may increase as users become dependent on the app the more they use it.\n",
    "- Due to their social media usage habits, the apps that should be pushed to this group should have a social element that allows them to communicate and be in touch with family and friends. All marketing efforts should be through social media, namely, Instagram, Youtube, Facebook, and potentially Tiktok.\n",
    "\n",
    "Tadwads:\n",
    "- This group has the second-highest revenue-generating potential, as they are not really price sensitive. They instead value apps that add value to their lives daily. They also tend to be thought leaders who can influence other people’s behavior and product adoption habits.\n",
    "- Marketing efforts would be better suited in fostering collaborations while engaging them and using their influence. For example, they could be used as beta testers in the apps and given premium access before everyone else utilizing the app. They can also be given coupon codes and vouchers to share with their database.\n",
    "- Influencer marketing has reached its peak, can build trust, rapport and have these influencers create content that will be more powerful than if it was coming from a company.\n",
    "\n",
    "Techy:\n",
    "- This group is very tech-savvy and very critical about what apps they invest in and the functionality of their application. However, they tend not to be too price-sensitive as they are willing to invest in apps that add value to their lives.\n",
    "- Marketing efforts should be invested in paid ad promotions on websites that perform product and tech reviews and blogs and websites that target people who strive to simplify everyday tasks.\n",
    "\n",
    "Wealthy-Parents\n",
    "- This is the bougie group, and they do not think twice about spending money and are lovers of nice things. However, they are probably older as we noticed that they do not utilize social media as frequently.\n",
    "- The best marketing tactic for this group would be to promote a sense of exclusivity. This type of group wants to be the first to have exclusive apps and gadgets.\n",
    "- Due to their tendency to be impulsive shoppers, we would recommend in-person events and one-day and limited-time offers to be pushed through to them. Creating an atmosphere to 'FOMO\".\n",
    "- For younger and older people in this demographic, we would recommend social media marketing through Tiktok(for younger), News apps(for older), blogs about high-end and premium products, and professional websites such as Linkedin.\n",
    "\n",
    "Self-Conscious\n",
    "- This group will only purchase it if they need it. They are not the type to just buy any apps they find useless. They are probably more rational than all the other groups. Marketing campaigns should be precise and highlight the benefits that these people will get. \n",
    "- As well as being very direct as these people do not use technology as much.Email marketing would probably be a better tactic for this group as they are barely on any social media platforms; however, everyone needs an email to function.\n",
    "- However, we also would not recommend tremedously trying to target marketing efforts to this demographic as they have the least revenue generation potential.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
